{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "#cargamos el modelo preentrenado en español\n",
    "ft = fasttext.load_model('cc.es.300.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funciones de análisis para encontrar palabras clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mariajosevelazquez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words_es]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def get_word_vectors_base(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = {word: model.get_word_vector(word) for word in words}\n",
    "    return word_vectors\n",
    "\n",
    "def get_word_vectors(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = {word: model.get_word_vector(word) for word in words if word not in stop_words_es}\n",
    "    return word_vectors\n",
    "\n",
    "#NO UTILIZA EL MODELO PREENTRENADO\n",
    "#obtiene la importancia de las palabras en el texto basandose en la frecuencia de las palabras\n",
    "def get_keywords_by_frequency(text, top_n=10):\n",
    "    text = preprocess_text(text)\n",
    "    words_base = text.split()\n",
    "    word_scores = Counter(words_base)\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return [word for word, score in sorted_words[:top_n]]\n",
    "\n",
    "#obtiene la importancia de las palabras en el texto basandose en la norma de los vectores\n",
    "def get_keywords_by_norm(text, model, top_n=10):\n",
    "    text = preprocess_text(text)\n",
    "    word_vectors = get_word_vectors(text, model)\n",
    "    \n",
    "    word_scores = {word: np.linalg.norm(vector) for word, vector in word_vectors.items()}\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    return [word for word, score in sorted_words[:top_n]]\n",
    "\n",
    "#obtiene la importancia de las palabras en el texto basandose en la similitud coseno  \n",
    "def get_keywords_by_similarity(text, model, top_n=10):\n",
    "    text = preprocess_text(text)\n",
    "    word_vectors = get_word_vectors(text, model)\n",
    "    \n",
    "    word_scores = {\n",
    "        word: np.mean([\n",
    "            np.dot(vector, other_vector) / (np.linalg.norm(vector) * np.linalg.norm(other_vector))\n",
    "            for other_word, other_vector in word_vectors.items() if other_word != word\n",
    "        ]) for word, vector in word_vectors.items()\n",
    "    }\n",
    "    \n",
    "    sorted_words = sorted(word_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    return [word for word, score in sorted_words[:top_n]]\n",
    "\n",
    "#algunas palabras vacias no incluidad en las stop_words\n",
    "palabras_vacias = ['asi','sino','gran']\n",
    "\n",
    "def preprocess_text_tfidf(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if (word not in stop_words_es and word not in palabras_vacias)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Obtiene la importancia de las palabras en el texto basándose en el modelo TF-IDF\n",
    "def get_keywords_by_tfidf(text, top_n=10):\n",
    "    text = preprocess_text_tfidf(text)\n",
    "\n",
    "     # Calcular la matriz TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    \n",
    "    # Obtener los índices de las palabras con mayor puntuación\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "     # Obtener las palabras correspondientes a los índices\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return [feature_names[i] for i in top_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas y funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_string(file_path):\n",
    "    try:\n",
    "        # Abrir el archivo en modo lectura\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Leer todo el contenido del archivo\n",
    "            file_content = file.read()\n",
    "        # Devolver el contenido como un string \n",
    "        return file_content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {file_path} no fue encontrado.\")\n",
    "        return \"\"\n",
    "    except IOError:\n",
    "        print(f\"Error: No se pudo leer el archivo {file_path}.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras clave por norma: ['mx', 'sr', '34', '32', 'tan', 'ven', 'mes', 'com', 'gota', 'daño']\n",
      "Palabras clave por similaridad: ['solo', 'así', 'aun', 'ahora', 'efectivamente', 'tiempo', 'lleva', 'igual', 'casi', 'llega']\n",
      "Palabras clave por frecuencia: ['fuga', 'agua', 'tejupilco', 'temascaltepec', 'potable', 'real', 'arriba', 'encuentra', 'mes', 'igual']\n",
      "Palabras clave por TF-IDF: ['fuga', 'agua', 'tejupilco', 'temascaltepec', 'reparar', 'arriba', 'real', 'potable', 'igual', 'mes']\n"
     ]
    }
   ],
   "source": [
    "#prueba texto 1\n",
    "\n",
    "file_path = 'posts/prueba1.txt'\n",
    "file_string = read_file_to_string(file_path)\n",
    "\n",
    "sample_post = file_string\n",
    "keywords1 = get_keywords_by_norm(sample_post, ft,10)\n",
    "print(\"Palabras clave por norma:\", keywords1)\n",
    "#regresa none\n",
    "\n",
    "keywords2 = get_keywords_by_similarity(sample_post, ft,10)\n",
    "print(\"Palabras clave por similaridad:\", keywords2)\n",
    "\n",
    "keywords3 = get_keywords_by_frequency(sample_post,10)\n",
    "print(\"Palabras clave por frecuencia:\", keywords3)\n",
    "\n",
    "keywords4 = get_keywords_by_tfidf(sample_post,10)\n",
    "print(\"Palabras clave por TF-IDF:\", keywords4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras clave por norma: ['0', 'da', '25', 'si', '396', 'zit', 'dio', 'ser', 'vez', '2019']\n",
      "Palabras clave por similaridad: ['denunciado', 'así', 'detenido', 'particular', 'informando', 'denuncia', 'presentado', 'ahora', 'detener', 'vez']\n",
      "Palabras clave por frecuencia: ['uniformados', 'queja', 'policías', 'policía', 'michoacán', 'joven', 'libertad', 'empresa', 'cedh', 'estatales']\n",
      "Palabras clave por TF-IDF: ['uniformados', 'queja', 'policías', 'joven', 'empresa', 'libertad', 'policía', 'michoacán', '1600', 'detenido']\n"
     ]
    }
   ],
   "source": [
    "#prueba texto 1\n",
    "\n",
    "file_path = 'posts/prueba2.txt'\n",
    "file_string = read_file_to_string(file_path)\n",
    "\n",
    "sample_post = file_string\n",
    "keywords1 = get_keywords_by_norm(sample_post, ft,10)\n",
    "print(\"Palabras clave por norma:\", keywords1)\n",
    "#regresa none\n",
    "\n",
    "keywords2 = get_keywords_by_similarity(sample_post, ft,10)\n",
    "print(\"Palabras clave por similaridad:\", keywords2)\n",
    "\n",
    "keywords3 = get_keywords_by_frequency(sample_post,10)\n",
    "print(\"Palabras clave por frecuencia:\", keywords3)\n",
    "\n",
    "keywords4 = get_keywords_by_tfidf(sample_post,10)\n",
    "print(\"Palabras clave por TF-IDF:\", keywords4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
